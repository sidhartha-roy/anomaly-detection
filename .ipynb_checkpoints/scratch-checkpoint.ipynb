{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the image data, dividing training, testing, and validation\n",
    "  \n",
    "  Use the inbuilt PyTorch dataloader from image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 87362 files [00:44, 1968.55 files/s]\n"
     ]
    }
   ],
   "source": [
    "# split data in folders\n",
    "import split_folders\n",
    "\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "split_folders.ratio('data', output=\"output\", seed=1337, ratio=(.8, .1, .1)) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 69889 images under train\n",
      "Loaded 8735 images under val\n",
      "Loaded 8738 images under test\n",
      "Classes: \n",
      "['anomaly', 'normal']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'datasets/'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "degrees = (0,180)\n",
    "\n",
    "# VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally. \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.3, saturation=0.3, hue=[-0.3, 0.3]),\n",
    "        transforms.RandomRotation(degrees,expand=True),\n",
    "        transforms.RandomAffine(degrees, translate=None, scale=None,shear=None, resample=False, fillcolor=0),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),  \n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=8,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "for x in [TRAIN, VAL, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "label_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "    \n",
    "# load dataset\n",
    "PATH = 'data/'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                               ])\n",
    "\n",
    "image_data = datasets.ImageFolder(PATH, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 87362\n",
       "    Root location: data/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = len(image_data)\n",
    "indices = list(range(data_points))\n",
    "split_index_tr = int(0.6*data_points)\n",
    "split_index_test = int(0.8*data_points)\n",
    "\n",
    "shuffle = True\n",
    "random_seed = 20\n",
    "batch_size = 20\n",
    "num_workers = 1\n",
    "pin_memory = True\n",
    "\n",
    "if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "train_idx, val_idx, test_idx =  (indices[:split_index_tr], \n",
    "                                 indices[split_index_tr:split_index_test], \n",
    "                                 indices[split_index_test:])\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        image_data, batch_size=batch_size, sampler=train_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    image_data, batch_size=batch_size, sampler=valid_sampler,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    image_data, batch_size=batch_size, sampler=valid_sampler,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "dataloader = {}\n",
    "dataloader['train'] = train_loader\n",
    "dataloader['val'] = val_loader\n",
    "dataloader['test'] = test_loader\n",
    "\n",
    "dataset_size = {}\n",
    "dataset_size['train'] = len(train_idx)\n",
    "dataset_size['val'] = len(val_idx)\n",
    "dataset_size['test'] = len(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5999977106751219\n",
      "0.20000686797463427\n",
      "0.1999954213502438\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx)/data_points)\n",
    "print(len(test_idx)/data_points)\n",
    "print(len(val_idx)/data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/Hvass-Labs/TensorFlow-Tutorials/\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # plot img\n",
    "        ax.imshow(images[i, :, :, :], interpolation='spline16')\n",
    "\n",
    "        # show true & predicted classes\n",
    "        cls_true_name = label_names[cls_true[i]]\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"{0} ({1})\".format(cls_true_name, cls_true[i])\n",
    "        else:\n",
    "            cls_pred_name = label_names[cls_pred[i]]\n",
    "            xlabel = \"True: {0}\\nPred: {1}\".format(\n",
    "                cls_true_name, cls_pred_name\n",
    "            )\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD1CAYAAADJYJnzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3wceX3/8dd3e5F2pVWzmou6u2zL3T7bd77ewoUQjhYgP0IJJSQhgZAcAdJ+QAK/EHKPhAAHgXCh3XGdcy/n3m1ZclOxetf2Pt/fH7MHysW+c5G0kvb7fDz00Ozs7Oxn97vznj4jpJQoiqJkEkO6C1AURZlsKvgURck4KvgURck4KvgURck4KvgURck4ppsZWAiR6buAB6WUBekuYrypdlXtOkNdt13VEt/NaU93AcqEUO06M123XVXwKYqScVTwKYqScVTwKYqScVTwKYqScVTwKZPMAHjAmgsFZrCIdBekZCAVfMrEcpqgKhfK7alfWwGwAOxlUGADmzHNBSqZSAWfMrGqPbClGublghlgFAEQdQFGsKqfoDL51K9OmViBKCQk2Gyp1VozEMUYzwajA7LM6a5QyUAq+JSJNRLV/2c7wGYCogg0DIlcSGSDzQBm9TNUJpf6xSkTKxiHSAIcNrCZAYFGEIgj4ll68KntfMokU8GnTKxYEqJJsBgh2wpGDZBoJDGE8sBoAsdNnTKuKLdNBZ8ysTTAGwKrCVxOMAggigUwBrMBk9rBoUw69YtTJt5oAIQAlx1sAkiSRGCMOQA7OIygDudTJpEKPmXiDYVBAlmO1Pa8MOBHRGyQcIDdCFa1nU+ZPCr4lIkXiOrB57JBtg2MSeIEMUqJMeIBsxHsajufMnlU8CkTLxCHSBxsdnBYwJQANARgCOSByQJZlnRXqWQQFXzKxIskIZgAm1U/rEWEkXhJEsISdIEwqeBTJpUKPmVyDIzqJ23kOsEsgQgJEmhhQDj0PbsmtYdDmRwq+JTJMeTTj+HLtoLTBqYwkiC2WC7EUmd12NTpa8rkmJDgK3KU8N5Fn6Miez3gmIi3UKab/iAkEuC0gN0MhhhxksS0KMa4Gyxmvb+iTIIJ2ZW2pfJBfvCJv8MXSHCk9Rw7zu1mz8XdNPWeYjSp7uuSkbwx/fQ1hwWcZjDGkQxhppJ42EkyBz0UB4LprlTJABMSfNlaJbv29VE8K4tV8+rZ2lCPFH9Ex/AQe08dYfu5X3Hg0h4ueRuB+ESUoEw14YR+BkdxPmRng3kAwkkShDAF8oiZreBWvwVlcox78OXaZrF20Uo6R7o50+Ej352Py+pkdmke5eV5vOe++3nPb99PKBDneHMjzx54iVfPvkBj7wkk0fEuR5lKBgMwtwgcZjBpIIJIGccUcABWsIbBYoCYlu5KlRlu3INvVcVmNq5poO1iD8uLi7lyYQRsPvp9got7e/DkuPHkOigpdbJxYT0bN9STiP4Fpy418fL+V3nl2AscbT9EnMB4l6ak20AAjALcdrAL8IWJJbzYw7kQt+qns1lMEIulu1Jlhhv34CtPLOHi0SGsBYLBET+1q4uwGw3EhIYt4qGz3U8wHuDsGR+NZ03kFjgoLLSzrHw+DR+dz19pn+Jsy2Ve2b+DFw+9wNEr+wnJ0fEuU0mHvoC+ZSPbkroGX4wkYUQUTGE3Ccegvp0voIJvwtjt2N/1GCSiSG+QRDyG0CSa0YjBKPTr5kSiCK8fzecHKSGpQTQGgSCEIxDVD0BHk/oZOdPQuAZfvq2ERxY+hjPpwN/mJycvC+/FJM5aK7acJMIhqav14G+JIswCU9JM39UwvV1JWi5LnA4zeYU26uZUsfj9VXzm/R/mUkc72w7u4sWDz3OweR8j8YHxLFmZTMNBfeJx2sFuAZMBLZHAjBFjMItEllEPPmXCWLOz+e2vfRNXTi4ACSRG9GtEGBFIJJIEFqlhSMQxahoyqRGLxogGQ8TDYbRoAk3GiEX8xHwBwiNeNG+AaCyJT4sTlUmIBEh4fYT7BwkPDBGJx/VxSw3N7yfu9ZEcGkVGE6nsTEAsCpFYKlgn1rgGn8PoAluUkqI8TKIIXzBC0B+m72wCi1lQtspB9GoM+xwTZqsZb3eIygeyiF/QkEIQj0P35SD9fQaEBrkeK6Vz5/Cxt7+fj73z/XR39bLz4B5+ceA5dp3bwWiibzzLVyZaJAmDQZjt0ffsWiIQ0YgSwuB1QqlFP59XmTBSSkJJDSNgBCwIDOjBpwECgQUzVgF2sxUz+jFvwqHfGM+IHhomfnMsXDLVbUqNJ5H6Hwci6AuF4VQ//X2SRKJRQuEIIqGRRCNGjFgwQNIbJDnsIxoKEwdCAgKxIPG+fhJXO0n6QySFIColCEHSIGjfto3oucab+h7GNfiuBpv57adXsrBwOfdWP8qm6rtYWFyPGTOBYJIr+8Jk2Uy4A2YSSY2SFTnEhjRkeRRHnoXRpjALHs4i4Zd4r8YJBhKcPRHB0mgmy2Eiv2gW79nyu7zngd+ls6efHSf28uLhF9l9fjsDkc7x/CjKRBnwQ2UxZGeBPaQvAWphrAE34aTQr95iNU3KXD8TJcxGTpqimElgwYABgSSJKRUFJsCiZwoGKTEKPRillFiFSIWlPpxM/Y8DVkAgsUiBSJ2AI8YMk0APTf09jBitDoTV8euwtAAi/zfhKFPDa0AIPVjj/CZ4BeABRoCf/NXn6E9n8AHEZYxTfYc41XeI/7sfFhQt597qh7i75j7qS5eTbbPiG4JI3MDV7Rout8BZY4chiafCTkJoaA5J/lorjvY45Q1WwsOS4fY47V1hms9KHFkW8vPyebzh7fzenW+nf2SEPaf288KJl9je+Cu6/a3j/bGU8dIzql+M1OkAgwYGP2EtiCuYD9IFtpB+rJ8KvgkhymYxHBqm0OFgNBjAbLSSn5WLQYtjFoKQTGI0WnBpEp/BgFFKkgKyEYSRWBAEgCgSJ0LfZIukHz0UEf9zs18EmXqsp6GGJI5IrVS/vrSpdyeRmBCppUaJEUESuBy4SjISoMw9C7PZyoiWwKzFqDblU4wgx2yl/ya/hwm/FtD5vhOc7zvB1/d/idq8hTxS/wj31z7IirkrcZgsBAMwehIwC5yjYHEZcVQKot0a9nIzQgrMUqNyoZ3wJSsJkWSkNcFgd4CWriSGBOQXOLhn/sP8zuqHGQ4FON56jOePvMArZ1/i0lAz03YL7Ew0FIBYBFxW/Rp8xhDGRBRz1IoI2ZEOoV+wYCSU7kpnJEe2i5pgnCVuycXBYcqsTox2B4lICIfFSWfYy3J3Ee0xP2utLs5rUUqFEbvBjA9BKZKDSDZjoAVJFlCM4ApQi2A/GnUIwuhLa2YEQ0AFkpPAHARtQC6CIJIAUIBgCEkOgktomAArBqKp6TaGgaysIuabPQwi8Yo4FqMdIzAfOO+w3/T3IKS88VAQQoxbgiyYtZg7q+7m0RUPsWL2KnLNThJRCEQgCjg9YCsDg0ViKBZEBySGAkATBC8nyKkyMXIiTiyepO9KFN9oEs0IuS4HhR4bbhdEiXGm8yzPn3yBV06/QGPPKTRua0niuJSyYXy+galjPNv1LVkM8IG1II2wuwk6NUSonhwW4V9+mUTBRbjqhaabnYffloxpV9udm/nwjl0EExHiWpJKi5OYlmSJwchpNFZgoAOoAGJSEkPgQnJZwEYEB5AsB9oR2JB4gE4EJUAbUIqkD8hC3/ZnRVAOnEHiBnoQuAAnEE8Fm4ZgGI0cDFwhiQlwY6RVauQhaBGCIiR+9FXuiIAECXIxMYCBM//4NY7/6Weu9RVct13TFnxjzcmt4q66u3hgyYNsqt5EfpYLIhAKQ1SCySYxFYJlNiT9YCqH6BBgl9htBgInJdZsyWhbnMGBGN6gvn3Q4bSS53KQn2sGJJeGmtl2/mWeP/ULjnccIXnzZ41kzAQyoR5ZAKV5sP8StIYgUEY2G4nPGyEy/zQMhOBYx2QuqGdMuzoefojq557Hgn6xHDNQhL4TohjwAflSkhCCwlR/G2CVkBCSQgQJoCC1epoHDCDJTa2welJLeFbABcSQFAOnENQCPUjmIuhFEkWwFDiNZD6Cc0hKEQwDdiRuKTgNLBdwFciSkBDgk5AjYBjIAf7qn77G9j+5ueCbEpe9bR+5zHcPXua7B/+N0ux53LVwC29b8TY21mwkz+mGkMB/FQKXwZwlsfaAMQ8s2QZkECzrINErsTmNzM/Jxn9GEvfFGR6O09UzxNVeSSSSIBYz82j1R3h3/cfoCbSw99IOtl18kaNXD+GPeNP9NWSOPh9UFuqHrliDEEwgZQSLL4tIUtMvUWUzQ1idwjbeDBb9QhCS39zmpBM9CIZT/3uEwAy0pIYxoG+WTaL3l4AttU1PH4fAir509/p4pZSYhL6rwpIaxwEgC8Ge1GsswOHU6/am/p8AvEA5gg6hB2wzgiiQL/QgdQjBIFCGvpTZ0XL5pr+HKRF8Y3X5W/nBoVZ+cOi7FLvKuWvhXTy4+GE2121kVmGBHoI9EOgAGiW2fIElB0x2gb3aiBaXWDZIHD4z2kVByeJCRlvjnGtu42pnD4FgALPNitlk4p7KD/KHWz/BqauXefw/HqPLdzbdHz8z9PkBAXarvoPD6CWW8OMIF4K0gDmuX6lFBd+4i5w8SeenPo4lIbEZDBjyc8GTg6GkEKvVSa5mxGwwkHDaMTodUFSAy+7CAWhSgsGAZrMisl3Y0ANEPwxG93q3EIJY6rF+F2U9MLVU97VOSkym/ozAafQXCAQxIWkjSTFGBAItNa484C6ACze/M3PKBd9YPb4OfnjwKX548CnyHEWsr76DR5Y/ypaaO6goLIeoYKQbAi0CzQr2ixJnqcCUC8IGntUmRExgqLSyYUEVnbu7KDZbKaSYxqYurnRdxZ5VzVAIBgM96f64mWMorF+0wGnUz+AwW0km/Nij1fjibjRzWD+eb1jt4BhviZY2hv75Wzc2sFGA04nRbOXXh5UbDODOxjhvNlkmOzZASAlWC6JqLsb8PBy2bOzCgEVKhNOBKC1Cs9sQLjcOsw2nNCCRGCxmEi4nRncuVrsTKeWvAzFpMqGl9hAHEBgwkUBfNY+hHx4zjL50GPfe/Hb7KR18Yw2F+nju9E957vRPcZpdrKvcwO+segd3Vt3JvNnlEBd4vdB7ApKahsMjcOYJnFWApuHwGIlVDvP2v/0gNblVfOm+f8LFLJxOIyODF4lqg+n+iJkjEANfWD+I2SzAECSJl0gyhDGUhZaLvtdXSa+kBF+AJAHCY/sPDMDllps/m15fFEwdsCLBbAaXE6MrB5vNiRGpH96iaYjZJZhrajBrgqQUSCmJWs0YKmejFeQhrVayzQ7Oaha8gze/I2zaBN9YwbiPbc0vsa35JRwmN2vmreOxFY+yte5uKssrkDEjw8PQ2ZtAnNdwOAVRQ4gNxjs4+Nnd/Oz8C3Sbr7Bxy2IsI4JtP38x3R8ps0hg2A8leWAyAAYMRLBjJRxwEs8V+nY+o9AnPmVmkICUvz6yj2gMBmIkBkb+d4g2NcGvdrzlKG/usOXfmJbBN1Yo4WXnpZfZeell7MYsVs5ZwwMLHuCehVupLluMQcLgKJw/047H7aYwUMLvF3wMd5mBSEeERKmkyXco3R8j8wwFYW5xagdHGC0cJagFMI+6icwx6gcxOyzgV5cqU8bftA++scLJAHtbtrO3ZTt/+aKF+pLlPLDoIdaVP0jZrAoKPC78kSQtHaPINo2G9fnsubyLU5dOpLv0zNMX0HdsOMyQDABBDIQx+B2g6Vdoxm5WwadMiBkVfGMlZIxjXYc41nUIwReYn7+Cu6ru4566h6gpXI7daMSRBd996vvpLjUzjYZA0yDbDtZRCGv4Y8O4QjkQtYIhqO/g6FfXZVTG34wNvrEkSc4PHuH84BG+eehL1HmWcXftA2xmE3vO7Ex3eZkpFNfPx3U59T27xjDgxZiwYAg50XK84MiIn6eSBhn5y2oePknzwZN88+DfpruUzJWQMJJaqjMDZkkirB/tZfI5iXkk2Ax6KMbVpeiV8aXuq6ukT68XzCb9MlTGCIhh4gQRow7QDPp5vbaMnDcrE0wFn5I+A36wWvTtfAYJIkiSCI5QAQiLHopqdVeZACr4lPQZDkEyngo3CeYQUYaJh5L6Dg5j6sKkijLOVPAp6eOP6efjZlv0kDNAghFs8RxENFs/0j9L3YNDGX8q+JT0SUjwh8GVrW/LEyE0vETwYxy1679OC3ooKso4UsGnpFfvqB5u0YR+60JGsOLANGCAAa9+hodJBZ8yvlTwKenVF9TvbBNNpC4Sp+GjB7PXDREBVgEO9TNVxpf6RSnpNRQEBHgcMDcX3BEEXozSDXn5kOcEh9rBoYwvFXxKegXiEEpAjhMcAogRx4eM62dwIKQ6pEUZdyr4lPRKShgNQ74DWoYh6AdC+qWLghb9wpev3/1aUcaJCj4l/Qb8+kHMZiM4TUj8SBJYht168DlMkKWSTxk/ah1CST9/BPKyYH4eaEY4FyIe8SONhbByEfiDEGsFb1+6K1VmCBV8SvoF4mDWwG2G+vng7SV2qQOrrxT29UJPn35oi6KMExV8SvqNhuHlZhiJwdVz0DmCRpJwMAFH012cMhOp4FPSL5KEFr/ePdKb3lqUjKB2biiKknFU8CmKknFU8CmKknFU8CmKknFU8CmKknFudq/uINA+EYVME3PSXcAEUe06M6l2vQ4hpZzMQhRFUdJOreoqipJxVPApipJxVPApipJxpn3wCSF2CyEabmJ4IYTYKYRwpR7fJ4S4IIS4LIT47JjhnhZCVE9EzYqipNe0D75b8ABwWkrpE0IYgW8B9wMLgMeFEAtSwz0J/FmaalRS3mzGJoT4mRCiItX9t0KIDiFE4A3DfFwI8YHJqFW5celu11sOPiHEs0KI40KIRiHEH4zpH0gVe1oIcUgIUZTqP0cIsUMIcSb1f3aq/1NCiCeFELuEEC1CiE1CiO8KIZqEEE+NGe+TQohjqff74jXq+X0hxNfHPP6QEOKfrlH6u4FfprpXAZellC1SyhjwNPBo6rl9wFYhhLqQwy2ayO9OCLEQMEopW1K9nkdvzzf6LvDJiaojE82Edr2dJb4PSilXAA3AJ4UQean+TuCQlHIpsBf4UKr/vwA/kFIuAX4E/POYceUCdwKfRv+gXwcWAouFEPWpYT4vpWwAlgCbhBBL3lDP08AjQghz6vEHgO9do+71wPFUdynQMea5zlQ/pJQacBlY+lZfxEwlhJibmgF9OzXDeVUIYU89V5+asZ0RQjwjhMhN9d8thPg7IcQe4FPjNWO7hrEzMKSUh6SUPW8cSEoZAtqEENeaeDKSatfbC75PCiFOA4eAcuD17WEx4IVU93Fgbqp7LfBfqe7/BDaMGdfzUj+g8CzQJ6U8mwqexjGvf4cQ4gRwEj0UF4x5PVLKILATeEgIUQeYpZRnr1G3R0qZugYS17ph69gDG/uBkmsMk0mqgW9JKRcCo8Bvp/r/APjz1IzsLPCFMa/JkVJuklL+Y+rxeMzY3mjsDOytHAM23uCwmSKj2/WWgk8IsRnYCqxNLdmdBGypp+PyN0dFJ7n+2SFjAyaa+q+N6X79sUkIMQ/4U+CuVIO8OOb9xvoP4P1cf2kPICGEeP1zd6KH9uvKgO4xj21A+DrjyRStUspTqe7jwFwhhBt9ItiT6v994I4xr/nvN4zjtmds11AMDNzgZ1AzsP8to9v1Vpf43MCIlDKUWrpacwOvOQC8M9X9bmD/TbyfCwgC3tQ2w/uvNZCU8jB6kL0L+PF1xnUBqEh1HwWqhRDzhBCWVH3PjRm2Br0RM9nYGdGbzcjGCl5nHLc7YxsrfAPDvE7NwP63jG7XWw2+V9A/2Bngy+iru2/lk8AHUq95L/CpG30zKeVp9DlGI/pGzdfeZPCfAK9JKUeu8/yLwObUeBPAx4FfAU3AT6SUjQCpgA1fa/tCppNSeoERIcTrqxnvBfa8yUveyg3N2N6gCai6wfHXAOdusbaMkUntelPn6gohMv3E3kEpZUG6ixhvql1Vu85Q123XTDyO73Zk8pUuZjLVrjPTddtVBZ+iKBlHBZ+iKBlHBZ+iKBlHBZ+iKBlHBZ+iKBlnigWfAEsu5tKHMNZ/Hywz9VYIiqKk0xS58oidYsI0GGwcu+dlhs2ribeFwfQCxNSRBoqijK80Bp8FqEbM/giGsnLyDvwWtVqYlgtn6dGKwT8ARnUdUEVRxl/6VnVNhbD4l0jbgyR9tXQa59IPzO76PiSDYBDg2AgiJ20lKooyM6Uv+BKdCGs72JzQdgGvWEkXMDt2kiytC0wWsM0B64q0lagoykQysHr+vzGn9ME0vHMayZZ/A3M2RNqQiSraMVOQCFIRegVsLohGwKIuo6YoM1G2+VEciffxSMML3LPliziLJ++907tXd+RVEH2QMx+yl9NtX8NloC64A0IDMNIDsh4MM+78cUXJaEZRSonjjxke7qOx2Uvpfe0s+jCYPTmAccLfP73BJ0dhZBt4aoAQIct6OoGy8CkK4sehtApmLYJstdSnKDPJrKwncFjmYbNkY6n/Jn353+XqMQcPr9hGbdk3AeuEvn/6j+Pr/wl4yiAxAt5OLmEhDtQajkOWB4xmyHkk3VUqijJOnKaH8Zi3YDNb8VSfpvi+r3DhECRPvI++ljm4ze9kbsGTE1pD+oPPvx8SVyB/DTCHIeroBapHXsCYHABNA0cDmGanu1JFUW6TyVhOoePT+PzD2LKC5Nz7WS72+rm6rY7R7uW09V9gbmkuUrzZtYZvX/qDjwgMvAAlS8G+hgSbaQbyYt0Uhl4DiwUMOeC6O92FKopym+YUfIXSkiUUFM5G1j+Bf9YRLm/3UOL7AkYcVJbVc77z32jv/86E1jEFgg/o/2+wC8jJB+oYpAAfsND3EggLJBLgvi/dVSqKchvyXR+iJOcBnEYXZctOU3TvDzi5A+Kn30dS5jFv1jLMlh4a2z4z4bVMjeALH4NQI+TPBWMBPaygF6jw7sSpdehVOpaC9a1u0KQoylRkNVVRXfJZrCaByT5A1tY/o6UT+rYvZnhQkkiEqJ07nxNXPoLE/9YjvE1TI/iQ0P1zKJgF2blAA1cAT8JLtf9lMNvA6AL3vekuNKPkrllHVmU1U+ZnokxbNWVfw2kuwm5w4lj9BP22s1z6lQdH/0cosGymofYRLvd+nZHAzkmpZ+r8ovt/ATIMOUVABZ3Mxg/MG3oWhAaJKOTcme4qM4fVwfrv/ZDHG8/xtqPHWf/kvzPvt9+JvbxCP6tGUW5Qcd5HmZWzFYu0k7P055g2focL+0A78QdkWapYXncvkeQ5zrZ9ftJqmiJXZwFizTC8F0rWQU8LvuhqznOVFaETZEUbCRjmg32Jvoc3dCzd1c541sVLSdTOY0SAs6GeJQ31rPjIh4j5gvhaW+natZu2PfsYajxD4Mol0JLpLlmZgmyWBVSXfg6ztGLL68a0+Y85dxZGdz2GM7aB0vIqcvJsvHL4D4HIpNU1dYIPoPsn0HAvePKQPQ308yLIEIv9L3IwfwkkDZBzjwq+SbAwkMXS7xxldO08opX5DNv0u0RbXU6cSxdRv3QRDX/0cbRwBG/TBdpeO0zXwf30HT3MaGsbJGPp/gjKFFBb9nVshgLM1gTu+z5GW6CTll9VYOl7Ozm5JVTOq+BMyz/gDe2b1LqmVvANvQSxfigogd4yWmUtQ5ykeuhlDhZ+AuImcN8F3V8D1IQ1kRY3X6T6Qx8iaptNtHYOhi33Eb2jhpG1lYzMMjAKJAC73Ub28qWsXr4Uyyf+gGQoTO/FK3QePkLXzm20Hj9JoK0NktE0fyJlspXm/zGFrg0IzYhl2VcYKnuetl+aMZ7/P9gtJVRXLWHAf5Tz7U9Mem1TK/i0XujZDsXvgKxLRPwbuMpJlkUuUBI+Qrd1C4g6cK6B4N50VztzGS2cN4InFsYUOUjJ6XMUnj6K7RuFOAuWE11cRmTDanq3FDC4bBYBNwwDScDusOOpX0R5/SKsH/4gvlCUvuZL9B3YR8uuXXQ2NuG73AzJRLo/pTKBnLZ66so/BwkD2ZUnsN/xZZqPQ+Dge8nSGqiuXYAjO8mOfR8F4pNe39QKPoCOn8O8d0FeHlqgigsyn9UMsmD4RbrL74aYAXIfVcE3gczltfi+/DR7zrdiObyLwiu9eDoamad1UjLQTc5OJyU791H8ZSvmsoWEV1cwcO8ChjeWMFJpx2uCodS4jA4rxcsXUbl8EXd8/KOEwjGGLjTTun07zQcP03PyBMHWK4BM50dWxpWBmvJ/wGJwY/NEcN/9aS71ROn51WrMA/dSUFLGvNkF7Dz2pwSjx9NS4dQLPv92RKAVWTob+loYitTTLLez0LuTPaX9xLVsyN0CXS6QvnRXOyOVbVnPwt9ZwNDIApK+++kdCtN7uYOeix0UHD5HQcsFcq9cwCF9zO5oQ3RIyn42iwrXbIzzqvDeUcvg+jL868roKTcQBAbRtxGa7BY89UuYU7+EjUA4EGbgzFmunDzO1cOH6di7D397S3q/AOW2VBZ/jmL3FtAMuFf/XwbtB2l/uRBLywdxOeewoG4+F66+SkvPP6atRiHljc9phRCTMlsWNf+CrP1DOLwbBnewVvt77iDJ87VPct7+NpBGaPkA+F+YjHLGOi6lbJjsN51ob2zXR195lY333s0I4JXgj8FQRL88YsQvMQ2GsZxqIe9iL8XHm7BeOElh3yAWeqigCAF4mI0rv5bwomKGN9cxsr6E7uX5+DwQRF8tTgBmwIl+LQ4T4PUF6D3bpAfh/r10HjhEqLMdpDaRX0FGtOtkcDlXsnbBy4hYDp66nZgevYcTB2Hg6c/hDGxl/tJVOLM0frF7JYnExYku57rtOiWDD8dm2LwLWs/DpSPkJb7M+2ihP28zP6r4McSM4HsGWj88KeWMkRETSMlHPsv833qI2voFFBfl4kj1DwDDGnhj4I1CKATJgMTUGyH7Yi+5Z1sp2nsS8+WjVPoNRLnCHMqxYCGPcrSyCoyr6+jcNA/v+gK6F9sZMesHMUTRg8+KfjcWK/pBpr5RP0OtLVw5dJjLO3bSffI4odaW8Q7CjGjXiWdh45K9ZJkbsBMR4GkAACAASURBVLtGcP/OZlp8jTT/4F5sLZ+kbO5Camrm8OqBj9E1OLFXX0mZZsGHFbH+ONI2Fw7tJCfyY7Ymf0y1NYt/X7idoWg5GAfh3BbQhienJF3mTCBGO8yuwrN8BXM2bWbRlo1U1lWQZ9KX0sKAD/AlwZsEbwSCAUj6wXJ5lNxL3biOnqf4XDuuS80UhyPYGCEPD7kUIyzZ2BYtYXBlHv1bKxhdXUhXuZEwesC+vjQoUv/tqW5vMMTA+Wau7NvP5YMHGT52hGDbba8aZ067TqCq0ieoK/0rZNKA576P4qv+d47+ZwnGQ18mz93AsmVLaGp7hkONj01WSdMt+EDM/VvE8r9AO3oUQ9cOlssvcr+M8PK8z3PM/WkQSWj/FAw/PVklQSZPINkFWJetovSuu6jdsJZF9YuY68ni9VtBBYB+YCgBI3EIJiASBBkC14UAzuarFO1uovByOzktV/BERsjBgAkopIZ4gQPT8lV41xXTuamQwaVuhnL08YbQV41NqT8L+tJgBIj4Agw1NtJy6CgdB19j4OBrhDo7ucmdJZnbruPE5dzAyppnsAoPnsW/QN7zOxzZAaHnniArcT9LV6wE6eXnu1eR1K5MVlnTL/iwrUZsPYgc6IazB6mI/gP3JI9jcS/hnyufh5gJojvg0vsmrSTUBJJihLmV5K9ZS9XGjcxfs4r5tdWUOG1Y0APJh75ndyQBo6nthFE/GEcgp91H3rkO8redpeBcM/buFmyMUEIBLpxkM4doZRbx5fPp2zyHzpV5DC/KZtSuH70ZQ18iNKT+rHpFxIARr4/L55rpP3SI/l3bGT11kmhX51t9INWut/U+NtbM347Lsorc8jZcb9/E8bYeep9+B/aODzO3dimV5Xk8u+eD9I18bzJKet00DD5MGNcfRhYsRzu0DUffszwg/5VaI/xw0cu0a/VgGIHGuyHRNVlFqQnkWkxWTJXVzFq5muo7NlK3vJ7K+bXMctiwo68WDwKDUl8iDEgISn0rha0zQfaBNjznr2DdewBrxwXKghpOQpRRTJIwswyL8C8sIryshsH7q+hd7GJ0vp0Rg76nOAyAxIBAAp0SsoQeiJGRUfqbL9Kzbx+DBw7ga27Cd6HpjZ9AtettqC77GyoLP4PDGcf58MP0uXZx5r8WYT/9BGUlDSxdNI+j55/lcOPbJqOcsaZj8IGh/AsYN/w18bPnoXk/SxJ/yUMMcGb2x3gh729AxqH7s9A/aXMRNYHcCKMZU00NxavWUbdyHYvXrWRebTV5DgtG9KDqAwYkDMX1nSWxBIghMF4cJOfIBWZdHMKz/yTm3iZKY0achCmnlAhJcqxVRFdVM7RmNp1bixiqyMVXZSUCjAItESi2SgxC/OYzAiWAd8dRvrF1Hfoy46+pdr1FLudGVtb8Aksil4JNXyW4/nOcfsUOv/oaVrGOdavqiUcG+c9XV5KUbRNdzhtdt12n3nF8Y2h9z2KOfR5RVIZsnUVvYjEd7GTO6Ms4PJ8gFMsC9z2TGXzKjUjGSTQ10tHUSMf3v802sxVrVR2ly5ZRu24DCzesZ05tFfU2E2ELjFr0wOp3wmhxPkPr8ukPgMX3ELbLA+S+1obz6HEKz3ZiHrjAvGiMyL5tzN5XQO1X3Vhy5tCxpZjA/Dn0bV6IbX4u2WWCQX6zaiyQFCK4uG8/bwg95RYJg5MFc/4BQyKXgoWHYM2XaD4BySOPYwnWM7+hhiw7/GT/Z9MRem9qSgcfsdNoXfsxlm8hkZvLSGg5HXIXW3ytzAnuosnyINhXgKUaYpfSXa1yPfEo0abTtDSdpuW/nuJliwNHdS3FDcuZt+EOahoaKK+uYJbTRtycOmwmC4Y8BoJlRbSvK4LIai52RXA2ddF8oAnzmRMUn+vEGDhL6WgLo8/0EnjGSs3f1bKofAHJ9YsJrptL9+oihqpyiXkE5UDzkSPp/jZmjMriz5FlXI7DPYj1zj+i1RdmdP9aDK2PMLd2DnNLHLx2+qd0DU3sZeRvxdQOPiDR/izmyi1QWEy8dwEDiVKG6WSp91c0FT8C0gm590KfCr5pIxYi1HiSK40nufL977DdbMc8r4LSNeuYt34DC9asprS2gtl2M3HACwxJGHHZ8NVUMvxIJYbRh2hr92I7dYHGQ42Yj+7D1NLC9/kpwx1Bfv/ptWx9ei5FFJOsLMe4aTmmVfn0njud7k8/I+RkbaY050OgCVzr/4YBxzFany9ENL2PgqIq6qpK6e/v5dC5T6e71Gua0tv4ADBUY73/BHFMaK9tI3f0P3iA55hlyeXfq1/Cr5VA4hRcenQyqlHbgiaDxY6tqpaSZSuo2LyRmnXrKampwGky4EW/III3BiMRiMVBSBg+P0TbT59nthYmEmkhsGcvD16xUUCAYpwUkcswIT7G9tTOkP9BtetNjddBQ/U2XIYVeBbuxPzYQ5w6quH72Z9jDzzKmnUNFOeY+f5L76Fv9EcTUcKNmp7b+ADQLiG79mCpe5CIZxZe7yIuyZcoj41QFdjBSft79Htx2BZB5Fy6q1XGQyxM5PwpWs6fouVH32G7zUFW3SLmrWyg5q67mNOwnLp5c9BcAj8wAhgX5tDlW0yvMLN04bsJfyLG4UMt5LVdYE73MO5j57hw4RDhCT3zLTNUljyBQyzB6u7CesefcK5FY2jXXTiGt7CgvpayfDO7jv4w3aH3pqZ+8AHxzmew1D4IhcVo3XX0RmoY4TzV3lc4mf0uEHbwPATdKvhmpEiIwKkjnD11hLPf/lfIcpNfVUPFlk2U37GZ0uX1LJldytKHVjAAXO0OELUKhmtdWO57FDHXRS3wwiPvhefVb+R2uBxbKM5+HyaRIGvjE/RYm+jaXwzN76Bo9jzmlXlo7ejkUNOfpbvUNzUtgk8OvkLSO4ChqATNWchgZAntnGex7zB58UsMyRpwbYXur6If46/MaAEvg6eOMnjqKEe+/jWE20N+XR3lmzZTtrKB6tUNOOaXk5if9+uDnZ/tHKH18pl0Vz6tGUQ2FQV/jSGai23Z9xip/hEXD4F25P0UZddTXVlFLA7bjn0a6El3uW9qWgQfdKF17EAsfifklxAaWUy39gyLtSiLAy+zO6sWjHPBUQ+h9FzfS0kf6R1m4PABBg4f4ARgysomt3455ZvuZOn6tZRu2cihg0cIN51Nd6nTWnn+n+EUy7CVnsZ0x1/RfAlCe96GPXwHFfW1FLgN7Dj6XYb8P0t3qW9pmgQfaB0/x7Tgnfpl6a/O5Wp4EX0cp3zkZQzZH0CLu8DzqAo+hUTAz8D+PQzs38MJwFk+G81gQV3s9NZl2+8k3/xuDJZBPPf/OZejQ3gP1GLqeIyKyjoq57i52HGF0y0TfzPw8TB1bi/5Vny7kcMdiLw8cJUQYhFNgDt0gZLYMUhoYFsP2NJdqTLFBDuuEm6/nO4ypi2DIYc57s9jwknOxn+m3bmHrgNOxOn3U+hZSFXVXIIh2HvqM+j73Ke+6RN8DJJsewHhAApKiIl6BvAQBOr8L+n33jXOhqxV6S5UUWaUMvdnsMYXk7doJ6Pzv8HlkxDY9zh2rYGKyhrsNth9/Jv4Qs+ku9QbNo2CD+h5BhkBCovBXkY/tfQDJaP7scgeiGjgeiDdVSrKjOG03o1bvB2zq5NQwxP0D0HwwCrso/dQVraAwkInZy6d4nL35N8p7XZMr+ALvYbsPAfZ2eAuJsQqOgFLvI/Zwf0Qi4FlNQh3uitVlGnPZPRQYvs0Bs1EouFv6LVeovPVPMSl36WoYD5V80oYHI5w8OxH0c+2nj6mV/ARgqvP6Z15xSCquUoZo0B5aDsQB1kAzvXpLFJRZgS36QPYkkuJzfshgdpfMHwQQsd/lyxLPXPn1ICAA2eeIKEdSnepN22aBR8w8N8wHIScMrDNJsQCOoHq4AGyRStEk5C1Nd1VKsq0Nxp9hiHzkwRWfoORq9D/6iayuJOaisXkuC0cb3qavtGvprvMWzL9gk87A937wGEBTwlR1tKBiaAWoTL0sn7ypnEZGArTXamiTGtJWuj2/Q0De0cYfKkci/Yu5hQuoKywgCudF2m8OjUvQHAjpl/wAfT+UD8cP38WGGbTTQUdQF5wFwgvJD3gWJPuKhVl+ktCpBmSg16c5mHmls/HF4aTl/8E6E13dbds2hzA/D8En4P+JsidD9ml4F3JEBepil/BlTiHL7EBHPdC4Ll0V6ooM4SPkfjnaeyMEIoYCMUn/Z7W42p6LvHhh/6f6rGdW4ZkMVfJJgq4o9tAkyCWg7E83YUqygyi0dr3Rfq8X0h3IbdtmgYfMPRLCMXAVQzGcoLU0guUxI5AchsM/T0kp9cudkVRJsf0XNUFSF4A7yXwLASbEy2YxWUgrvVD6HPprk5RlCls+gYfQej6fzBaCpH/BK4QSHdJiqJMC9M4+AD/t8Gf7iIURZlupu82PkVRlFukgk9RlIyjgk9RlIyjgk9RlIyjgk9RlIyjgk9RlIxzs4ezDALtE1HINDEn3QVMENWuM5Nq1+sQUqo7TymKklnUqq6iKBlHBZ+iKBlHBZ+iKBlnRgSfEGK3EKLhOs/9TAhRker+WyFEhxAi8IZhPi6E+MBk1KrcmDdr0+sML4QQO4UQrtTj+4QQF4QQl4UQnx0z3NNCiOqJqFm5cemeZtMefEKICbtQghBiIWCUUrakej0PXOuO498FPjlRdSiT4gHgtJTSJ4QwAt8C7gcWAI8LIRakhnsS+LM01TgjzIRp9raDTwgxVwjRJIT4thCiUQjxqhDCnnquXghxSAhxRgjxjBAiN9V/txDi74QQe4BPCSGeEkI8KYTYJYRoEUJsEkJ8NzXep8a815NCiGOp9/niDZT3buCXrz+QUh6SUva8cSApZQhoE0Jc6wvOKEKIZ4UQx1Pf8R+M6R9IzX1Pp9q0KNV/jhBiR6qNdwghZqf6j0ubCiF+Xwjx9TGPPySE+KdrlD62rVcBl6WULVLKGPA08GjquX3A1omceKc6Nc3qI7itP2Au+q1/6lOPfwK8J9V9BtiU6v4S8I1U927gX8eM4yn0H6dA/4H6gMXowXx8zLg9qf/G1DiWjBlfwzVq2wMsvkb/wDX6fR74k9v9Pqb735jv2A6cA/JSjyXwcKr7K8BfprqfB34v1f1B4NnxbFPACVwBzKn+B67Tpu1Adqr77cB/jHnuvcC/jHm8DViR7u86jW2c8dPseK3qtkopT6W6jwNzhRBuIEdKuSfV//vAHWNe899vGMfzUv80Z4E+KeVZKaUGNKI3FMA7hBAngJPAQvTVmDdTDAzc4GfoB0pucNiZ7JNCiNPAIaAceH17WAx4/Q4zx/lNm6wF/ivV/Z/AhjHjuu02lVIGgZ3AQ0KIOvQAPHuNuj1Sytevziiu8fzYA1ZVW2f4NDtei/vRMd1J9KWFtxK8zji0N4xPA0xCiHnAnwIrpZQjqcVp21u8R/gGhnmdLTV8xhJCbAa2AmullCEhxG5+8/3FUz9y0Nv4er+dsQEzXm36H8BfAM3A967zvgkhhCE14XWih/bryoDuMY8zvq3J8Gl2wnZuSCm9wIgQYmOq13vRF2NvlQv9i/emti/dfwOvaQKqbnD8NeirdpnMDYykQq8OuJGbEx8A3pnqfjew/ybe74baVEp5GD3I3gX8+DrjugBUpLqPAtVCiHlCCEuqvrH3Gq1BXypRxsikaXai9+r+HvBVIcQZoB59m8EtkVKeRl9cbkTfo/PaDbzsRWDz6w+EEF8RQnQCDiFEpxDir8cMux7Yfqv1zRCvoM+pzwBfRl/dfSufBD6Qes17gU/d6JvdZJv+BHhNSjlyned/3dZSygTwceBX6BPST6SUjQCpCTAsr7HBXAEyZJq9qXN1hRCZfmLvoJSyIN1FjDfVrqpdZ6jrtmvaj+ObZjL5ShczmWrXmem67aqCT1GUjKOCT1GUjKOCT1GUjKOCT1GUjKOCT1GUjKOCT1GUjKOCT1GUjJOxl+ZRpibznCLMDUuRze1EW9vQQtG3fpEy9VltsHYLXG1H+IeQA31pLUcFnzKlFD+2Ecujm8gWNhyGOKYTvXS/sI9L23aluzTldtTUIP7kk4i2Lqz5HiI//SkOt43gT38GAf9bv36cqVVdZUqRdWX0nW2ib6AdR2kxWz/5aR5/9Wke2/FTln7w9zG73OkuUbkVy5YjQyG0wQHCly7j+MMPMevJb+H5+bMY3/NhyCue1HJU8ClThrWyhHiOE/+2o9jidiqdpbhiQZ45/zwjy+ws/uZHsOar4Jt2zGZYvx6OnYLGC9DWTl5ZGdHBYUb7+0luXgcL5k9qSWpVV5kysu5YTKTfC54sosSYV1hBq78H79AAmI10791FoOVqustUbtaCRZDrgaHTIJM41q3EWJhPorUd7dQZGByEYzdyIaDxM+OX+MzL7sWw4p1vPaCSdo4NSwhcaCN/eS3L1q4nR7PQ2HcBhAGX283wi4fTXaJyKzZtht5+cGVDXS2WwkIwGuk/3wQI6O6CcGhSS5rRwWdd9Qizvv4ctg9/hZwH/jDd5ShvwrZgNqIom8TFFvyxMOXOIlrEEMOhUbJz3Bh9IbwHMv06sdNQlgtWr4FzzXCuCUORB/eqFSSGhkl0dYOmwZVLk17WjA0+65bHcf71T+g934Upy0LJF76Oedkj6S5LuQ7XnfUEfGGIS1Y2rKIgp5Denj4uXL4IRjN9O44T6xtNd5nKzVrRANkuGBqFeBJrVhbuPA/DTc3Q0gZ9PdDW8pajGW8zMvgcj34K++d+wOihY8RPHsF3/gr9vT7ufPlZSh7/YLrLU97IZCR7/UKCQyFMD27CkJVFtslJf3SYguIiLHYLvb/Ym+4qlVtguOdu8PvA7YYVSyjcvB4ZjYI/BCYLtFwBLTn5dU36O04w1//5Bxx/9A28B44iE1FwuvAsLGf5HW5iNkHpF/4NR2V9ustUxnAunoehLJ9o/wjllXNYsXgdmozRHe2jaG45Hs2M73BzustUblZhEaxYBm0dYLFgnF9NvsNJ2Ocn2N8HBXlwtTUtpc2g4DOQ+/mnsH7gzxn88S+Q7S2YKhdRvnwuyx4sZ2jYwOnDUa5c1Zj1xR9idM1Kd8FKSu79KwkFYwinjdyKInJMTs4G24lGI+R4PAy+chTpfeMNvpSpzrB5EziyoH8YSOKaXUbY4cDb2QXhCAwN6Ds20mBmHM5idVH0d08j7rifvr2N0NUFniJKymHVlvkkYnD6UoLhs52QTBAYOAMyku6qFUBYTHjurOdKSz9Z5TmsXrAeu8GKLziEyWrEYbbQ9ct96S5TuQXG+x4kMTQC0SiiphpHjhunzcaFqx36DShPnkhbbdM/+FyFlH/rlyTmr6FnbyP092NYv47FDy9hRZ2J1gCcOJLAe7EPkjHY8z1iL3813VUrKdkraxDF+QSPXKF2wVJcQQ3NnaDb34crNxf/hXYG95166xEpU0tlJWLRIuS+Q+CLYIrbyaqah29wkOTgIMQjcD59d/ic1qu6YlYds3+wm8SCNfTsaYLOHowlpdTdM59Fi8xcHRYc3z6C9+A5/XzAV/8fqNCbUooeXo9/yAfmJAvm1jLLPZvO8CCRRIS8ggJ6nn0NwrF0l6ncJOP9D6CZjPpeW5mDLTePUk8u3oFBfTW3tRWGh9JW37Rd4jPXrmbut39JKKuInu1nYNSHdcl86pbnsGixg+FhuNQOvhENgqPw7Dfh1C/SXbYyhrCZyNm0lAvNbbjLZyENBqya5MJoC06nEzsmtZo7HQmB4a47SXR2gTcIQ4KcFVUM+AP4r3aCMKR1NRem6RKfpeEhKn+0jaC1iK5XT0Msgr22grX3lrBqkZP+ETh8Ikn7hRBEvPDcF1XoTUGulTXIAjeBIS+llRXMzZ9NwpAglozjzssjeKadkaPn012mcpPE0npERSWypQ00G9b6lRQX5SATSYK9/TAyDBea0lrjtAs+xz0fZN5TzzMczKJ7x0nwBnBVlbPxoRIWzBJ0DsOhQ3GG+2KInlb4xuNwYXe6y1auYdZvbcTbNwpSMKewnLJENpcjvQSTYdyOHK4+uxu0TL8n9vRjuv9BtHAc+oegM4zLYyRoMdHf1g7RKJw/D6HJPUXtjaZV8Lne+cfM/dZ3GOiJ07/3BMSS5KxdyPo78qnxQGMHHDoWwz+iYbh6Br76GLQcSXfZyjUIpwX3xkUMXOrEUZ7PrLwChD2L/tgIVocVQzRKxzPqGnzTjt2O2LyBZEcHeH1Qt5jsO+aTn+Nm9GonxCJw6ni6q5w+wZf70a9R9vf/SFdTgOG9xzBlWyjdupDNW3MoLDRzvpv/3969Rzd53gcc//50s4Vt2ZZtMNgGzJ0QDClgCCGEpCEtbdKsl3Xduqyn22l3ttM127rtrPfbadLLznpZt6ynaZcup20asiZNSJqk4ZJCKFBuBmxMML7bkizZlizJuuvZHxKtDzXBItivbD+fczg8evXqfX/WT/rpfd/ned+X0+cSDPstmFsPkX7onSjv60aHrV1F2bYGlLOUgG+Q+pUrqU4Vk2IUV8xLeZmTwIk2/GfajA5Ty5Fpy23IwsWo7i4YEaxdPqwLSunq7Sc+OJi5EkvH1J+i9gdxGh3ANUkB1V/5MTWf/AQ9x/0EjjWBs5LFW1ewY6udahtc6odDxxIEwhZsR/+P1NfeDSHjeoy0a6u+/3YC3mEshUWUOkspd1QwnAiRJkm5tZyO3b8yOkTtOlju2Ul6JAQeD5jKKNzSQKXTQdzvh1gSTp+GZNLoMPO8V9daxcJHfkbxrjtpPzjIaOtFTNXzWLSlni1rzZQBBy9Ba3OSlLJifvE7xH/woNFRa9dgchbj3L6OcyebmbOglHn2UuZh51j0HBaTmXQkSu8vXzM6TC1XVXOxbN9GpKMTlBnbxiVUNy5iMBjG19Wd3c01tjf3srzd4pPKepbt3kflu+6k59AgoxfaMC9YwLLtS9neYCaUhL0t0NqaJp0E0xNfIKWL3rRQde8mkqU2gt4AVbULWOJcTDd+AvEQ5RUVeI+dI3xBX3B0ujHfs5N0RQXqUidYrTjuWo/dWQBKER/0Q3e3YaeoXSkvC5+pZh1rnvwVjq03c2HfIMHzbVBTw5rtdWxbK4zGoPlCmrZLoJIp+N5HSOz+otFhaxNU996d+Pt9FJYU0bhqM/MpoSPaS0ziFFPEpcf3GB2idh0K3nUfKhiG4SC2hjXYRkZQ9kJcHV2Z3dvf5k9HY94VPuvNO1n9zH7My5fS+vIA4XMXsdbXsu6OWjYvEeIxON6q6OoxYRkdIf2NPyGx9zGjw9YmyFZXRdmGFXjb+rDMLaHAkiJmFvzxCPaCQhJDAfpeOmx0mFqOZMVKzOvXE7/UBakk1oV1lJSXgcnM6IAPQiNw7ozRYf5OXhW+wm3v4+bdz5F2ltPyy25GL7Th2LyUdVvms6UegnHYf0rR6xLMQ/0kPv8OkkeeNjpsLQdz37qBlNmE3zfEzavWsbqgFnMqgT8dpKyoHNfeY0R7vUaHqeXIes9OVEEhqqsbU10N5lCQcoeDYDhEIhiE1tbMwOU8kTeFr+hdD7L68d2E0gVcfLGd9EAfxQ3LuektVdy6woTLDwdPpfD4BEtPC9FP30XydX0AfLqpfc8OBnr6SaeFeEGSuEnREu0hEY9ixULn7peNDlHLldmCddfbibvcMODFNG8upXU1+P0BAm4PRGNw0vixe2PlReFz/MUXWfqdbzHkhbaXz5P0DGCvX8rG7VXcNA88I3D0TJo+jxlT06+JfupuVP8Fo8PWclS4pJryTatwtXZQtmgBjuJy3LEAvoSfkhIHKfcw/fvy5ziQNjHSsA7zihUk2jrBZMLqLKUgFsdc6mDEPZAZ2tKcX/dLMbjwCc4H/4P6r3yOge4kXfubUANDlDWuYcPtc7mlKrN1/Mz+JINBE4XHnyL+pV2ogMvYsLXrUr1zE0mriZHhICH3JWqjFmw2K8H4KCX2UvpfOUx8cMToMLUc2e69F2Wxonr6kaoqylcvJ5lIEgkGSY2MQGsLxPLr+pfGjeMTO+WfeZwFH30v/RfieH9zBouKU755JatX2XlLLRxqV7ScjZNK2TC9+F2ijz5I5gqG2nRU8547cHf2kRzwURIeZMncpfTEhkgl41gxcenn+4wOUctVUTHWu3YQ73eDz4dt4y2Iy838NTdx/tjJzP02jk7tPXMnwpgtPms5zq89Q9WH34ureRTv4VMQjDKnqorGLZXU1Vo42JLmfEuSSNyMPPEZEo/+HbroTV/21XWUNq7G0z0AA0E2rriNalsl/VEfRaUOot1uXK/o3dzpxrR5M7JoEfGObkhDobOEAvscvL39BL1e6OuH9vw79XDqC591HuUPPYvjj+7B1RRg8PBpGPRTWDOfW+5bzhw7nGtPc7wpSXg4gul7f0vyqYemPEztxpp79yYSCMMd/RRvX0fdB+6jK+7FHx6irKicrmd/TSpo7BU7tNxZd+0ilUigevqgooyUEspLHSRRpPwBaDpldIjjmtpd3aLFlD78NPYd6/H81kvkbBMMh7Fv3MDGndXML4bXziXp6UhhjgdQ3/oQyTMvTmmI2uSouX8b3q5eEvE0lYV2GuYsojc9jFJAOkW7Pjd3+qmoxHzrFuIeH3iHMC+vZ8GiRbj7XQTDYRgcghPHjY5yXFO3xVdUj+PhX2C7fT3e431EjhwGX5CKTau5aVMVJUUWfvFKkJ6OKOahHtTD95PWRW9GmLO6jtJ1S+k5246ppIC1O24jZI5zwttMUVkR/vOXGDxy1ugwtRzJttth7lxS3T0QiSBlpaQDI5QsWUzY7YWuDhj0GR3muKam8DlvpujrL2BrbGDotYskjh8BsVC2aiELG5cwmlAcPhQgMipIVwvpL7+D9MX8OyCqXZ+579xKOJ1kuMuDc2EVC21VRJIRwvEoRXMc9D19EOLGX7FDy41l59tIR+Oozh6oLKeivo6Y2YSvvZPUZIjqPwAACN5JREFU8HBenaJ2pckvfDW3Yf/q81jWrmLwcAups6chMELxohqWvXsDo5EkvR0R/IMgrYdQD9+L8l6c9LC0qVNz71YGO12kR0YpslmoK3bSNuImlU5hi0Pvzw8YHaKWq9qFmBsaSA4MgMeHFJcQ9w3jXDA/c6aG2w0X8/cm8JNc+Aow/803SS9byMihU6izJ6HldZxbNrLkbWshEqWzxU/QE0OOP4f6tz+GkD5daSYpWFmDdVk17vNdUGKnrrYeMzZ8o5neXP/ZNvyn9WD0aWfzraQdxaR6XBCLY50/D2ddLd7efiIuF7x+AcL5exP4SS58MVI/+Syxl/ajzp6G003Mf/9bWbx9JaMDEY7/spfYKHD4R6j/fgDiockNR5ty5VvXEE2l8Hf5IBZhblEp0WQUT8CHo9RB77O/Bn1bjemnsZFEPIbq7kMsZsxlxYx6BlBWS2ZMn8F3UbuWye/VbXkJPH2Y1ryP6r/+APPfuoGL+y8xMhSHAjs8/2144auTHoZmDOc9jfi6PKQCQWruWEtJWTlHe5uw2qxYYil6nz5gdIharhbUwepVqAEfuNyouvnMsduJCYRfv5QZu9fVYXSUb2hqOjcGz2GJnMaxaQNtTx1hxBWEdBJ++kld9GYwS42T4oYlDLb2wEiYCoudefYKgtEQtiI7gZOthJo7jQ5Ty9W6W6DCCf0uCIaw1tUSC49icZSQ8LjhfDOk8ruzasqGsyRPPEP35/6eQLgAEhH48T/DsSemavWaARzbG0jazAQ6fFBSTP3aNbT6O/EODVDiKKHnKX0XtWlpcyPEY9DZC4WFqGiUstoaosNDpD0D0JJfFyQYz5QNYE6nYfT5b0MoCt2noePoVK1aM0jZHesZ7veRdnmodJaRikMoOAwFFlRgFNee3xgdopYrZxWsXgnDfhjwgdVKQXUVg80XSKkUtLeDx210lNc09aesvfo9XfRmAfM8B0XrluA73weeYQqrHSQkRUtbC47KCoYPnyPWNWB0mFqubtkAVZXQ54aRUXAUYw1HMM+tJN7Vk9nNnQby+y5r2rTluGs9idI5BFs7KJxTyOKtm/F192G2mikoLKTt6VeNDlG7HnfememF7/dAPIr9LQ3EQyGi7gFwufLyggTjyYsLkWozT8ITwP3zg9Dej2mOiYql9fS7erAVFZHyBggcyJ/7L2gTJALNZ+BUE7gHkIU1pPrdmKw2VDgM589DKGh0lBOit/i0SRHa1wT7msAs2HZtpnXPK8QkhHPxGgIvHCfhDRgdopYrpeCnP4bdT0KpE9ZvwLR6BaGjJ8Fmhvbpc8aVLnza5Eop/HuO4N9zBBZVYG7zMHqoxeiotDcjmYBBD2rvC8SajsPSZWCygrvf6MgmTJSa+LB5EZntY+xPKKU2Gh3EjabzqvM6Q101r/oYn6Zps44ufJqmzTq68GmaNuvowqdp2qyjC5+mabOOLnyaps06uvBpmjbr6MKnadqsk+uZGz6gazICmSYWGR3AJNF5nZl0Xq8ipzM3NE3TZgK9q6tp2qyjC5+mabOOLnyaps06M6LwicgBERn3Kgwi8pSILMm2vyIiPSISumKej4nIh6ciVm1i3iinV5lfRGSfiDiyj98uIhdEpE1E/nXMfE+IyPLJiFm7tnzJq+GFT0Qm7ZqAIrIGMCul2rOTngMax5n1h8DHJysObUq8A2hSSo2IiBn4T2AXcBPwpyJyU3a+R4B/MShGLXeTktc3XfhEZLGInBeR74tIs4i8LCL27HPrReSIiJwRkadFpDw7/YCIPCQirwIPishjIvKIiOwXkXYRuUNEfphd7mNj1vWIiBzPrueLEwjvg8AvLj9QSh1RSrmunEkpNQp0ish4RXFWEZFnRORE9j3+6JjpoewWc1M2p/Oy0xeJyN5sjveKyMLs9BuSUxH5KxH55pjHHxGRfx8n9LG5bgTalFLtSqk48ARwf/a5g8Ddk/mDm490Xq+glHpT/4DFQBJYn338JPDn2fYZ4I5s+0vAt7LtA8B/jVnGY9k/QrJ/yAiwlkxhPjFm2c7s/+bsMhrGLG/jOLG9CqwdZ3ponGmfBj7xZt+P6f5vzHtsB84BFdnHCrgv2/468Jls+zngQ9n2XwLP3MicAkXAJcCanX74KjntAkqy7fcBj4557gHgu2Me/wrYYPR7rfNqXF5v1K5uh1LqdLZ9AlgsIqVAmVLq8u20fgRsH/Oan12xjOdUJvqzgEcpdVYplQaayRRXgPeLyEngFLCGzObuG5kPeCf4NwwACyY470z2cRFpAo4AdcDl4yZxYE+2fYLf5+RW4CfZ9uPAtjHLetM5VUqFgX3AvSKyiswX5ew4cTuVUpfvdCPjPD92wOpszLXO6xg3anM/NqadIvOrci3hqywjfcXy0oBFROqBfwI2KaWGs5vVhddYR2QC81xWmJ1/1hKRHcDdwK1KqVEROcDv379E9sMOmRxf7bMz9oN4o3L6KPApoBX4n6usNykipuwXsJfMl/uyWmDsDSFmVa51Xv/QpHVuKKUCwLCI3J6d9ACZXc/r5SBTLAPZ4xC7JvCa88CyCS5/BZldgNmsFBjOfjlWAVsm8JrDwAey7Q8Ch3JY34RyqpQ6SuYD/2fAT6+yrAvAkmz7t8ByEakXEVs2vmfHzLuCzNbJbKHzeoXJ7tX9EPANETkDrCdznO+6KKWayGw2N5PphX1tAi97Hthx+YGIfF1EeoE5ItIrIl8YM+9twCvXG98M8SKZX+wzwJfJ7BZdy8eBD2df8wDw4ERXlmNOnwReU0oNX+X53+VaKZUEPga8RObH70mlVDNA9osYUeN0cs1gOq9XmNHn6kqmd3k/cJtSKvUG890C/KNS6oEpC07LiYjsAb6plNp7lefnA/+rlNp5jeX8AzCilPrBJISp5ciovBo+jm8yKaUiwOeBmmvMWgl8dvIj0nIlImUi8jqZX/NxvxwA2V/670t2oOsb8JPpaNMMZHReZ/QWn6Zp2nhm9BafpmnaeHTh0zRt1tGFT9O0WUcXPk3TZh1d+DRNm3X+H1jGL/CoMCfVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_sample = True\n",
    "if show_sample:\n",
    "    sample_loader = torch.utils.data.DataLoader(\n",
    "        image_datasets[TRAIN], batch_size=9, shuffle=shuffle,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "    data_iter = iter(sample_loader)\n",
    "    images, labels = data_iter.next()\n",
    "    X = images.numpy().transpose([0, 2, 3, 1])\n",
    "    plot_images(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Linear(in_features=25088, out_features=4096, bias=True)\n",
      "ReLU(inplace=True)\n",
      "Dropout(p=0.5, inplace=False)\n",
      "Linear(in_features=4096, out_features=4096, bias=True)\n",
      "ReLU(inplace=True)\n",
      "Dropout(p=0.5, inplace=False)\n",
      "Linear(in_features=4096, out_features=1000, bias=True)\n",
      "39\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 126,002,664\n",
      "Non-trainable params: 12,354,880\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "# Explore the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg16 = vgg16.to(device)\n",
    "\n",
    "ct = 0\n",
    "for child in vgg16.children():\n",
    "    #print(child)\n",
    "    if isinstance(child, Iterable):\n",
    "        for layer in child:\n",
    "            ct += 1\n",
    "            print(layer)\n",
    "            if ct < 28:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "    else:\n",
    "        ct += 1\n",
    "    \"\"\"\n",
    "    if ct < 7:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \"\"\"\n",
    "print(ct)    \n",
    "\n",
    "summary(vgg16, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25088\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Remove all the linear layers from the vgg16 model\n",
    "# Make the last two convolutional layers trainable\n",
    "# add linear layers\n",
    "num_features = vgg16.classifier[0].in_features\n",
    "print(num_features)\n",
    "\n",
    "\n",
    "features = list(vgg16.classifier.children())[:-7]\n",
    "print(features)\n",
    "\n",
    "#vgg16.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = []\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 1000]      25,089,000\n",
      "             ReLU-34                 [-1, 1000]               0\n",
      "          Dropout-35                 [-1, 1000]               0\n",
      "           Linear-36                 [-1, 1000]       1,001,000\n",
      "             ReLU-37                 [-1, 1000]               0\n",
      "          Dropout-38                 [-1, 1000]               0\n",
      "           Linear-39                    [-1, 2]           2,002\n",
      "       LogSoftmax-40                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 40,806,690\n",
      "Trainable params: 28,451,810\n",
      "Non-trainable params: 12,354,880\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.63\n",
      "Params size (MB): 155.67\n",
      "Estimated Total Size (MB): 374.87\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# First remove all the fc layers from the VGG19 model\n",
    "#features = list(vgg16.classifier.children())[:-6]\n",
    "print('features =',features)\n",
    "features.extend([nn.Linear(num_features, 1000)])\n",
    "features.extend([nn.ReLU(inplace=True)])\n",
    "features.extend([nn.Dropout(p=0.5, inplace=False)])\n",
    "features.extend([nn.Linear(1000, 1000)]) # Add our layer with 2 outputs\n",
    "features.extend([nn.ReLU(inplace=True)])\n",
    "features.extend([nn.Dropout(p=0.5, inplace=False)])\n",
    "features.extend([nn.Linear(1000, len(label_names))])\n",
    "features.extend([nn.LogSoftmax(dim=1)])\n",
    "vgg16.classifier = nn.Sequential(*features)\n",
    "\n",
    "vgg16 = vgg16.to(device)\n",
    "summary(vgg16, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, criterion):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    \n",
    "    print(\"Evaluating model\")\n",
    "    print('-' * 10)\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        for i, (data, target) in enumerate(dataloaders[TEST]):\n",
    "            # Tensors to gpu\n",
    "            if use_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Validation loss\n",
    "            loss = criterion(output, target)\n",
    "            # Multiply average loss times the number of examples in batch\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            accuracy = torch.mean(\n",
    "                correct_tensor.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples\n",
    "            test_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        test_loss = test_loss / len(dataloaders[TEST].dataset)\n",
    "\n",
    "        # Calculate average accuracy\n",
    "        test_acc = test_acc / len(dataloaders[TEST].dataset) \n",
    "    \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Avg loss (test): {:.4f}\".format(test_loss))\n",
    "    print(\"Avg acc (test): {:.4f}\".format(test_acc))\n",
    "    print('-' * 10)               \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "optimizer = optim.Adam(vgg16.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eval_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ab925f88171c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate model before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test before training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate model before training\n",
    "print(\"Test before training\")\n",
    "eval_model(vgg16, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          save_file_name,\n",
    "          max_epochs_stop=50,\n",
    "          n_epochs=100,\n",
    "          print_every=2):\n",
    "    \n",
    "    # early stopping initilization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    valid_max_acc = 0\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Main loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # keep track of training and validation loss each epoch\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "\n",
    "        # Set to training\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        \n",
    "        # Training loop\n",
    "        for ii, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # tensors to gpu\n",
    "            if use_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Predicted outputs are log probabilities\n",
    "            output = model(data)\n",
    "            \n",
    "            # Loss and backpropagation of gradients\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # Calculate accuracy by finding max log probability\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            \n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "            \n",
    "            # Track training progress\n",
    "            print(\n",
    "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.',\n",
    "                end='\\r')\n",
    "\n",
    "        # After training loops ends, start validation\n",
    "        else:\n",
    "            model.epochs += 1\n",
    "            \n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Validation loop\n",
    "                for data, target in val_loader:\n",
    "                    # Tensors to gpu\n",
    "                    if use_gpu:\n",
    "                        data, target = data.cuda(), target.cuda()\n",
    "                        \n",
    "                    # Forward pass\n",
    "                    output = model(data)\n",
    "                    \n",
    "                    # Validation loss\n",
    "                    loss = criterion(output, target)\n",
    "                    # Multiply average loss times the number of examples in batch\n",
    "                    valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    _, pred = torch.max(output, dim=1)\n",
    "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                    accuracy = torch.mean(\n",
    "                        correct_tensor.type(torch.FloatTensor))\n",
    "                    # Multiply average accuracy times the number of examples\n",
    "                    valid_acc += accuracy.item() * data.size(0)\n",
    "                    \n",
    "                # Calculate average losses\n",
    "                train_loss = train_loss / len(train_loader.dataset)\n",
    "                valid_loss = valid_loss / len(val_loader.dataset)\n",
    "                \n",
    "                # Calculate average accuracy\n",
    "                train_acc = train_acc / len(train_loader.dataset)\n",
    "                valid_acc = valid_acc / len(val_loader.dataset)\n",
    "                \n",
    "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "                \n",
    "                # Print training and validation results\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\n",
    "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                    )\n",
    "                    print(\n",
    "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
    "                    )\n",
    "                    \n",
    "                              # Save the model if validation loss decreases\n",
    "                if valid_loss < valid_loss_min:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), save_file_name)\n",
    "                    # Track improvement\n",
    "                    epochs_no_improve = 0\n",
    "                    valid_loss_min = valid_loss\n",
    "                    valid_best_acc = valid_acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                # Otherwise increment count of epochs with no improvement\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    # Trigger early stopping\n",
    "                    if epochs_no_improve >= max_epochs_stop:\n",
    "                        print(\n",
    "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "                        )\n",
    "                        total_time = time.time() - overall_start\n",
    "                        print(\n",
    "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
    "                        )\n",
    "\n",
    "                        # Load the best state dict\n",
    "                        model.load_state_dict(torch.load(save_file_name))\n",
    "                        # Attach the optimizer\n",
    "                        model.optimizer = optimizer\n",
    "\n",
    "                        # Format history\n",
    "                        history = pd.DataFrame(\n",
    "                            history,\n",
    "                            columns=[\n",
    "                                'train_loss', 'valid_loss', 'train_acc',\n",
    "                                'valid_acc'\n",
    "                            ])\n",
    "                        return model, history\n",
    "                    \n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    # Record overall time and print out stats\n",
    "    total_time = time.time() - overall_start\n",
    "    print(\n",
    "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "    )\n",
    "    print(\n",
    "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
    "    )\n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "    return model, history\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained for: 0 epochs.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cb2fe9fc320b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vgg16_anomally.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model, history = train(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvgg16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4a83eb8ff935>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, save_file_name, max_epochs_stop, n_epochs, print_every)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Predicted outputs are log probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Loss and backpropagation of gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomally/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    339\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 341\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    342\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "save_file_name = 'vgg16_anomally.pt'\n",
    "model, history = train(\n",
    "    vgg16,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders[TRAIN],\n",
    "    dataloaders[VAL],\n",
    "    save_file_name=save_file_name,\n",
    "    max_epochs_stop=50,\n",
    "    n_epochs=100,\n",
    "    print_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.load_state_dict(torch.load(save_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "----------\n",
      "\n",
      "Evaluation completed in 0m 42s\n",
      "Avg loss (test): 0.4742\n",
      "Avg acc (test): 0.8045\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "eval_model(vgg16, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/dataset/'\n",
    "print len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
